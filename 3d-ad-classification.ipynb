{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8086625,"sourceType":"datasetVersion","datasetId":4773615},{"sourceId":8221702,"sourceType":"datasetVersion","datasetId":4874179}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **3D Binary Classification Models for Alzheimer**","metadata":{}},{"cell_type":"markdown","source":"# (1) Import the necessary packages","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nimport math\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nimport time\nfrom scipy.ndimage import zoom\nimport csv\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD\nfrom torch.utils.data import TensorDataset, DataLoader\nimport torch.nn.init as init\nfrom torch import optim\n\nimport numpy as np\nimport nibabel as nib\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nfrom glob import glob\nimport torch\nfrom skimage.measure import label, regionprops\nfrom scipy.ndimage.morphology import binary_fill_holes\nimport scipy\nfrom sklearn.metrics import roc_curve, auc\nimport nibabel as nib\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check that CPU is available \ncuda = torch.cuda.is_available()\nprint(\"GPU available:\", cuda)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Set random seed for reproducibility\nnp.random.seed(0)\ntorch.manual_seed(0)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# (2) Data Exploration","metadata":{}},{"cell_type":"markdown","source":"### Count samples for each label for training and validation","metadata":{}},{"cell_type":"code","source":"#Checking the number of samples in each folder (train, validation and test)\n!ls /kaggle/input/preprocessed-data/ADNI_large_sample/train/T1_affine/AD/ADNI_*.nii| wc -l\n#!ls /kaggle/input/preprocessed-data/ADNI_large_sample/test/T1_affine/AD/ADNI_*.nii| wc -l\n!ls /kaggle/input/preprocessed-data/ADNI_large_sample/validation/T1_affine/AD/ADNI_*.nii| wc -l\n\n!ls /kaggle/input/preprocessed-data/ADNI_large_sample/train/T1_affine/CN/ADNI_*.nii| wc -l\n#!ls /kaggle/input/preprocessed-data/ADNI_large_sample/test/T1_affine/CN/ADNI_*.nii| wc -l\n!ls /kaggle/input/preprocessed-data/ADNI_large_sample/validation/T1_affine/CN/ADNI_*.nii| wc -l","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load sample","metadata":{}},{"cell_type":"code","source":"test_path = '/kaggle/input/preprocessed-data/ADNI_large_sample/validation/T1_affine/AD/ADNI_005_S_0929_MPR_m06_g_AD_dx_3_age_82.7_scan0_mri_brainmask_mni152brain_affine_tl.nii'\ntest_img = nib.load(test_path)\nprint(test_img)\narr = test_img.get_fdata()\nprint(arr.shape, type(arr))\nprint(test_img.affine)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualize three axis: coronal, axial, and sagittal","metadata":{}},{"cell_type":"code","source":"arr.max(), arr.min()\nf, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(10,5))\nax1.imshow(arr[:,109,:], cmap='gray')\nax2.imshow(arr[:,:,91], cmap='gray')\nax3.imshow(arr[91,:,:], cmap='gray')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test and visualize downsampling factor","metadata":{}},{"cell_type":"code","source":"from scipy.ndimage import zoom\n\ndef resample_img(arr, val=2):\n    new_arr = zoom(arr, (1/val, 1/val, 1/val))\n    print(new_arr.shape, arr.shape)\n    return new_arr\n                         \nout = resample_img(arr,2.5)\n\n\nf, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(10,5))\nax1.imshow(out[:,out.shape[1]//2,:], cmap='gray')\nax2.imshow(out[:,:,out.shape[1]//2], cmap='gray')\nax3.imshow(out[out.shape[1]//2,:,:], cmap='gray')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# (3) Upload data and create data loader","metadata":{}},{"cell_type":"markdown","source":"### Define MRIDataset class","metadata":{}},{"cell_type":"code","source":"#Creating DataLoader for each set (test, train and validation) using batches\nclass MRIDataset(Dataset):\n    def __init__(self, DataDir, mode, input_T1, transform=None,T1_normalization_method = 'max', downsample=2.5):\n        print('***************')\n        print('MRIDataset')\n        self.DataDir = DataDir\n        self.input_T1 = input_T1\n\n        if input_T1:\n            self.T1_img_files = sorted(glob(DataDir + 'T1_affine/*/*.nii'))\n            print(f'T1 path: {DataDir}T1_affine')\n            print(f'Load T1. Total T1 {mode} number is: ' + str(len(self.T1_img_files)))\n        \n        self.transform = transform\n        self.T1_normalization_method = T1_normalization_method\n        self.downsample = downsample\n\n    def __len__(self):\n        return len(self.T1_img_files)\n    \n    def resample_img(self,arr, val=2):\n        new_arr = zoom(arr, (1/val, 1/val, 1/val))\n        return new_arr  \n    \n    def __getitem__(self, idx):\n        label = self.T1_img_files[idx].split('/')[-2]\n        label = (1 if label == 'AD' else 0)\n\n        current_T1 = None\n        #T1_dimension = (182, 218, 182)\n\n        if self.input_T1:\n            current_T1 = nib.load(self.T1_img_files[idx]).get_fdata().astype(np.float32)\n\n        # normalization (important !!!!)\n        assert self.T1_normalization_method in ['NA', 'max', 'WBtop10PercentMean']\n\n        if current_T1 is not None:\n            if self.T1_normalization_method == 'max':\n                current_T1 = current_T1 / current_T1.max()\n            elif self.T1_normalization_method == 'WBtop10PercentMean':\n                current_T1_BM = self.WB_to_brain_mask(current_T1)\n                normalization_factor = \\\n                np.mean(current_T1[np.logical_and(current_T1 >= \\\n                    np.percentile(current_T1[current_T1_BM == 1], 90, interpolation = 'nearest'), \\\n                    current_T1_BM == 1)])\n                assert normalization_factor > 0\n                current_T1 = current_T1 / normalization_factor\n                      \n        \n        if self.downsample is not None:\n            current_T1_new = self.resample_img(current_T1,self.downsample)\n            \n        if self.input_T1:\n            sample = {'T1': current_T1_new,\n                      'label': label,\n                      }\n            \n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Transform samples to Tensor objects","metadata":{}},{"cell_type":"code","source":"class ToTensor(object):\n    def __call__(self, sample):\n        torch_sample = {}\n        for key, value in sample.items():\n        \tif key == 'label':\n        \t\ttorch_sample[key] = torch.from_numpy(np.array(value))\n        \telse:\n        \t\ttorch_sample[key] = torch.from_numpy(value)\n\n        return torch_sample","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create Datasets","metadata":{}},{"cell_type":"code","source":"# define batch size\nbatch_size = 1\n\n# define dataset dir\nTrainDataDir = '../input/preprocessed-data/ADNI_large_sample/train/'\nValidationDataDir = '../input/preprocessed-data/ADNI_large_sample/validation/'\n\nnormalization_method = 'max'\n\n# Train\nTrain_MRIDataset = MRIDataset(DataDir = TrainDataDir, mode = 'train', input_T1 = True,transform=transforms.Compose([ToTensor()]),\n                        T1_normalization_method = normalization_method, downsample = 1.5)\n\n# Validation\nValidation_MRIDataset = MRIDataset(DataDir=ValidationDataDir, mode = 'validation', input_T1 = True,transform=transforms.Compose([ToTensor()]),\n                                T1_normalization_method = normalization_method,downsample = 1.5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Store Datsets (to speed up training)","metadata":{}},{"cell_type":"code","source":"def preprocess_and_save_dataset(dataset, save_path):\n    \"\"\"\n    Preprocess a given dataset and save the processed data.\n    \"\"\"\n    os.makedirs(save_path, exist_ok=True)  # Ensure the save directory exists\n    \n    preprocessed_samples = []\n    for idx, sample in enumerate(dataset):\n        # Assuming your sample is a dictionary with 'T1' and 'label' keys\n        preprocessed_samples.append(sample)\n        \n        # Optionally, save individual samples to reduce memory usage\n        #torch.save(sample, os.path.join(save_path, f'sample_{idx}.pt'))\n\n    # Optionally, save the whole dataset as one file (if memory allows)\n    torch.save(preprocessed_samples, os.path.join(save_path, 'dataset.pt'))\n    \ndef load_preprocessed_data(preprocessed_path):\n    \"\"\"\n    Load preprocessed data from the given path.\n    \"\"\"\n    return torch.load(os.path.join(preprocessed_path, 'dataset.pt'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocess and save the training dataset\npreprocess_and_save_dataset(Train_MRIDataset, '/kaggle/working/train/')\n\n# Preprocess and save the validation dataset\npreprocess_and_save_dataset(Validation_MRIDataset, '/kaggle/working/val/')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocessed_train_data = load_preprocessed_data('/kaggle/working/train/')\npreprocessed_validation_data = load_preprocessed_data('/kaggle/working/val/')\n\n# Convert preprocessed data to DataLoader if needed\n# Note: This step might not be necessary if you directly iterate over the preprocessed samples\nTrain_dataloader = DataLoader(preprocessed_train_data, batch_size = batch_size,\n                       shuffle=True, num_workers=4)\nVal_dataloader = DataLoader(preprocessed_validation_data, batch_size = batch_size,\n                       shuffle=False, num_workers=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# (4) Define model: 3D VGG","metadata":{}},{"cell_type":"code","source":"class VGG(nn.Module):\n    '''\n    VGG model \n    '''\n    def __init__(self, feature_extractor, input_T1):\n        super(VGG, self).__init__()\n        self.input_T1 = input_T1\n        self.feature_extractor = feature_extractor\n        self.T1_feature_dimension = 4608 #Changed from 1*128*5*6*5 to make it match x = x1.view(x1.size(0), -1)\n        feature_dimension = self.T1_feature_dimension\n        self.classifier = nn.Sequential(\n            nn.Dropout(),\n            nn.Linear(feature_dimension, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(True),\n            nn.Linear(4096, 2),\n         )\n\n         # Initialize weights\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.kernel_size[2] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                m.bias.data.zero_()\n\n    def forward(self, x1):\n        x1 = self.feature_extractor(x1)\n        #print(x1.shape)\n        x = x1.view(x1.size(0), -1)\n        #print(x.shape)\n        \n        x = self.classifier(x)\n        return x\n    \ndef make_layers(cfg, input_T1, batch_norm=False):\n    layers = []\n    in_channels = 1\n\n    for v in cfg:\n        if v == 'M':\n            layers += [nn.MaxPool3d(kernel_size=2, stride=2)]\n        else:\n            conv3d = nn.Conv3d(in_channels, v, kernel_size=3, padding=1)\n            if batch_norm:\n                layers += [conv3d, nn.BatchNorm3d(v), nn.ReLU(inplace=True)]\n            else:\n                layers += [conv3d, nn.ReLU(inplace=True)]\n            in_channels = v\n    feature_extractor_T1 = nn.Sequential(*layers)\n    return feature_extractor_T1\n\ncfg = {\n    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n    'E': [16, 16, 'M', 32, 32, 'M', 64, 64, 64, 64, 'M', 128, 128, 128, 128, 'M', \n          128, 128, 128, 128, 'M'],\n}\n\ndef vgg16(input_T1):\n    return VGG(make_layers(cfg['D'], input_T1), input_T1)\n\ndef vgg16_bn(input_T1): #with batch normalization\n    return VGG(make_layers(cfg['D'], input_T1, batch_norm=True), input_T1)\n\ndef vgg19_bn(input_T1):\n    \"\"\"VGG 19-layer model (configuration 'E') with batch normalization\"\"\"\n    return VGG(make_layers(cfg['E'], input_T1,batch_norm=True),input_T1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualize model structure\n# model = vgg16(input_T1=True)\n# model = vgg19(input_T1=True)\nmodel = vgg19_bn(input_T1=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# (5) Define helper functions and paramters for training and validation","metadata":{}},{"cell_type":"markdown","source":"### Helper Functions","metadata":{}},{"cell_type":"code","source":"#The performance on validation set has not improved for a while. Early stop. Training completed.\nclass EarlyStopChecker:\n    '''\n    Early stop checker. Credits to Chen \"Raphael\" Liu and Nanyan \"Rosalie\" Zhu.\n    '''\n    def __init__(self, search_window = 5, score_higher_better = True):\n        self.search_window = search_window\n        self.score_higher_better = score_higher_better\n        self.score_history = []\n\n    def __call__(self, current_score):\n        self.score_history.append(current_score)\n        if len(self.score_history) < 2 * self.search_window:\n            return False\n        else:\n            if self.score_higher_better == True:\n                if current_score < np.max(self.score_history) and np.mean(self.score_history[-self.search_window:]) < np.mean(self.score_history[-2*self.search_window:-self.search_window]):\n                    return True\n                else:\n                    return False\n            else:\n                if current_score > np.max(self.score_history) and np.mean(self.score_history[-self.search_window:]) > np.mean(self.score_history[-2*self.search_window:-self.search_window]):\n                    return True\n                else:\n                    return False\n                \nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\ndef accuracy(output, target, topk=(1,)):\n    \"\"\"Computes the accuracy@k for the specified values of k\"\"\"\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define loss function, early stop checker, and scheduler","metadata":{}},{"cell_type":"code","source":"#Defining the required variables\ncuda_idx = '0'\nmomentum = 0.9\nweight_decay = 5e-4\nlr = 1e-5\ndevice = torch.device(f\"cuda:{cuda_idx}\" if (torch.cuda.is_available()) else \"cpu\")\n\n# Define loss function (criterion) and optimizer\ncriterion = nn.CrossEntropyLoss()\ncriterion = criterion.to(device)\n\n# Initialize the checker for early stop\nearly_stop_checker = EarlyStopChecker()\n\noptimizer = torch.optim.SGD(model.parameters(), lr,\n                            momentum=momentum,\n                            weight_decay=weight_decay)\n#Defining adaptive learning rate\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.8, patience = 2, verbose = 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Move model to GPU","metadata":{}},{"cell_type":"code","source":"model.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define train and validation functions","metadata":{}},{"cell_type":"code","source":"data_dropout = True\ndata_dropout_remaining_size = 80\ninput_T1 = True\n\ndef train(train_loader, model, criterion, optimizer, epoch, device):\n    \"\"\"\n        Run one train epoch\n    \"\"\"\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    train_display_counter = 0\n    model_output_list, AD_ground_truth_list = [], []\n    metrics_to_save = []\n\n    # switch to train mode\n    model.train()\n\n    end = time.time()\n\n    # If we use data-dropout, we randomly sample a certain subset for training in each epoch.\n    if data_dropout:\n        print('The size of the training set is :', len(train_loader), '. We will randomly sample %s scans for this epoch.' % (data_dropout_remaining_size))\n        valid_training_indices = sorted(np.random.choice(range(len(train_loader)), data_dropout_remaining_size, replace = False))\n\n    for i, (data) in enumerate(train_loader):\n        if data_dropout:\n            if not i in valid_training_indices:\n                continue\n\n        # assign variables\n        input_data_T1 = None\n        if input_T1:\n            input_data_T1 = data['T1'].unsqueeze(1)\n        target = data['label']\n\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        if input_T1:\n            input_data_T1 = input_data_T1.to(device)\n        target = target.to(device)\n\n        # compute output\n        output = model(input_data_T1)\n\n        #print('output: ', output, 'target: ', target)\n        loss = criterion(output, target)\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n\n        optimizer.step()\n\n        output = output.float()\n        loss = loss.float()\n\n        # measure accuracy and record loss\n        acc1 = accuracy(output.data, target)[0]\n        losses.update(loss.item(), batch_size)\n        top1.update(acc1.item(), batch_size)\n\n        # measure sensitivity, specificity, AUC.\n        model_output_list.append(output.data.cpu().detach().numpy().tolist())\n        AD_ground_truth_list.append(target.cpu().detach().numpy().tolist())\n\n        AD_prediction_list = [scipy.special.softmax(pred)[0][1] for pred in model_output_list]\n        fpr, tpr, _ = roc_curve(AD_ground_truth_list, AD_prediction_list)\n        operating_point_index = np.argmax(1 - fpr + tpr)\n        sensitivity, specificity = tpr[operating_point_index], 1 - fpr[operating_point_index]\n        AUC = auc(fpr, tpr)\n        \n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        torch.cuda.empty_cache()\n    \n    print('\\nEpoch: [{0}][{1}/{2}]\\t'\n                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                      'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n                      'Accuracy@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n                      'Sensitivity ({sensitivity:.3f})\\t'\n                      'Specificity ({specificity:.3f})\\t'\n                      'AUC ({AUC:.3f})'.format(\n                          epoch, i, len(train_loader), batch_time=batch_time,\n                          data_time=data_time, loss=losses, top1=top1,\n                          sensitivity=sensitivity, specificity=specificity, AUC=AUC))\n    metrics_dict = {\n                'Epoch': epoch,\n                'Batch': i,\n                'Time_Value': batch_time.val,\n                'Time_Average': batch_time.avg,\n                'Data_Value': data_time.val,\n                'Data_Average': data_time.avg,\n                'Loss_Value': losses.val,\n                'Loss_Average': losses.avg,\n                'Accuracy@1_Value': top1.val,\n                'Accuracy@1_Average': top1.avg,\n                'Sensitivity': sensitivity,\n                'Specificity': specificity,\n                'AUC': AUC\n                }\n    metrics_to_save.append(metrics_dict)\n    return metrics_to_save","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validate(val_loader, model, criterion, device, scheduler = None):\n    \"\"\"\n    Run evaluation\n    \"\"\"\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    val_display_counter = 0\n    model_output_list, AD_ground_truth_list = [], []\n    metrics_to_save = []\n\n    # switch to evaluate mode\n    model.eval()\n    end = time.time()\n    with torch.no_grad():\n        for i, (data) in enumerate(val_loader):\n            \n            # assign variables\n            input_data_T1 = None\n            if input_T1:\n                input_data_T1 = data['T1'].unsqueeze(1)\n            target = data['label']\n\n            # measure data loading time\n            batch_time.update(time.time() - end)\n\n            if input_T1:\n                input_data_T1 = input_data_T1.to(device)\n                \n            target = target.to(device)\n\n            # compute output\n            with torch.no_grad():\n                output = model(input_data_T1)\n                loss = criterion(output, target)\n\n            output = output.float()\n            loss = loss.float()\n\n            # measure accuracy and record loss\n            acc1 = accuracy(output.data, target)[0]\n            losses.update(loss.item(), batch_size)\n            top1.update(acc1.item(), batch_size)\n\n            # measure sensitivity, specificity, AUC.\n            model_output_list.append(output.data.cpu().detach().numpy().tolist())\n            AD_ground_truth_list.append(target.cpu().detach().numpy().tolist())\n\n\n            AD_prediction_list = [scipy.special.softmax(pred)[0][1] for pred in model_output_list]\n            fpr, tpr, _ = roc_curve(AD_ground_truth_list, AD_prediction_list)\n            operating_point_index = np.argmax(1 - fpr + tpr)\n            sensitivity, specificity = tpr[operating_point_index], 1 - fpr[operating_point_index]\n            AUC = auc(fpr, tpr)\n            \n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            torch.cuda.empty_cache()\n            \n        metrics_dict = {\n                'Batch': i,\n                'Time_Value': batch_time.val,\n                'Time_Average': batch_time.avg,\n                'Loss_Value': losses.val,\n                'Loss_Average': losses.avg,\n                'Accuracy@1_Value': top1.val,\n                'Accuracy@1_Average': top1.avg,\n                'Sensitivity': sensitivity,\n                'Specificity': specificity,\n                'AUC': AUC\n                }\n        metrics_to_save.append(metrics_dict)\n        print('\\n * Accuracy@1 {top1.avg:.3f}'.format(top1=top1))\n        print(' * Sensitivity {sensitivity:.3f} Specificity {specificity:.3f} AUC {AUC:.3f}'\n            .format(sensitivity=sensitivity, specificity=specificity, AUC=AUC))\n\n        scheduler.step(loss)\n\n        return top1.avg, AUC, metrics_to_save","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# (6) Training Loop","metadata":{}},{"cell_type":"code","source":"warnings.filterwarnings('ignore')\nstart_epoch = 0\nepochs = 100\nbest_acc = 0\nbest_AUC = 0\n\nmetrics_to_save_all_epochs_train = []\nmetrics_to_save_all_epochs_val = []\nfor epoch in range(start_epoch, epochs):\n    # train for one epoch\n    metrics_to_save_train = train(Train_dataloader, model, criterion, optimizer, epoch, device)\n    metrics_to_save_all_epochs_train.extend(metrics_to_save_train)\n\n    # evaluate on validation set\n    current_acc, current_AUC, metrics_to_save_val = validate(Val_dataloader, model, criterion, device, scheduler)\n\n    # remember best acc@1 and save checkpoint\n    is_best = best_AUC < current_AUC\n    best_acc = max(current_acc, best_acc)\n    best_AUC = max(current_AUC, best_AUC)\n\n    metrics_to_save_all_epochs_val.extend(metrics_to_save_val)\n\n    if early_stop_checker(current_AUC):\n        print('The performance on validation set has not improved for a while. Early stop. Training completed.')\n        break\n\n# Save training metrics \nkeys = metrics_to_save_all_epochs_train[0].keys()  # Get keys for the CSV column names from the first dictionary\ncsv_file_path = '/kaggle/working/training_metrics_train_downsample2.csv'  # Specify the Kaggle output path\nwith open(csv_file_path, 'w', newline='') as csvfile:\n    writer = csv.DictWriter(csvfile, fieldnames=keys)\n    writer.writeheader()\n    for data in metrics_to_save_all_epochs_train:\n        writer.writerow(data)\n\n# Save validation metrics\nkeys = metrics_to_save_all_epochs_val[0].keys()  # Get keys for the CSV column names from the first dictionary\ncsv_file_path = '/kaggle/working/training_metrics_val_downsample2.csv'  # Specify the Kaggle output path\nwith open(csv_file_path, 'w', newline='') as csvfile:\n    writer = csv.DictWriter(csvfile, fieldnames=keys)\n    writer.writeheader()\n    for data in metrics_to_save_all_epochs_val:\n        writer.writerow(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# (7) Testing the model# Test data","metadata":{}},{"cell_type":"markdown","source":"### Create test dataset","metadata":{}},{"cell_type":"code","source":"TestDataDir = '../input/test-data/test/'\n\ntest_dataset = MRIDataset(\n    DataDir=TestDataDir, \n    mode='test', \n    input_T1=True,\n    transform=transforms.Compose([ToTensor()]), \n    T1_normalization_method='max', \n    downsample=1.5\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Store dataset, load it, and create Dataloader for testing","metadata":{}},{"cell_type":"code","source":"#preprocess_and_save_dataset(test_dataset, '/kaggle/working/test/')\npreprocessed_test_data = load_preprocessed_data('/kaggle/working/test/')\ntest_dataloader = DataLoader(preprocessed_test_data, batch_size=batch_size,\n                            shuffle=False, num_workers=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define test function","metadata":{}},{"cell_type":"code","source":"def test_model(test_loader, model, device):\n    \"\"\"\n    Run the model on the test dataset and evaluate the performance\n    \"\"\"\n    model.eval()  # Set the model to evaluate mode\n    total_predictions = []\n    total_targets = []\n\n    with torch.no_grad():  # Turn off gradients for testing, saves memory and computations\n        for i, data in enumerate(test_loader):\n            input_data_T1 = data['T1'].unsqueeze(1).to(device) if input_T1 else None\n            labels = data['label'].to(device)\n\n            outputs = model(input_data_T1)\n            probabilities = torch.softmax(outputs, dim=1)\n            predicted = probabilities[:, 1]  # Assuming the second column represents the positive class probabilities\n\n            total_predictions.extend(predicted.cpu().numpy())\n            total_targets.extend(labels.cpu().numpy())\n\n    # Calculate performance metrics\n    fpr, tpr, thresholds = roc_curve(total_targets, total_predictions, pos_label=1)\n    AUC_score = auc(fpr, tpr)\n    sensitivity = tpr[np.argmax(1 - fpr + tpr)]\n    specificity = 1 - fpr[np.argmax(1 - fpr + tpr)]\n    accuracy = np.mean(np.array(total_targets) == (np.array(total_predictions) > 0.5))\n\n    print(f'Accuracy: {accuracy:.3f}\\nSensitivity: {sensitivity:.3f}\\nSpecificity: {specificity:.3f}\\nAUC: {AUC_score:.3f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Print test results","metadata":{}},{"cell_type":"code","source":"test_model(test_dataloader, model, device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}